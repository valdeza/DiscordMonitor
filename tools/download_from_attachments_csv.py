"""Downloads all Discord attachment links in specified .csv file generated by extract_log_attachment_urls_to_csv.py .

This Python script appends random characters to the end of downloaded files out of the expectation that many attachments will have the same name. Current progress will be output to console as each file is downloaded:
msgid 123123: attid 321321 -> attachment_filename.ext
	(1/3 eta~1:23:45~5:43:21)
(Information in parentheses only displayed if --verbose specified)
"""

from datetime import datetime # for ETAs
from pathlib import Path # for path manipulation for output directory
import argparse
import csv # for reading input .csv URLs file
import os # for output directory validation
import random # for generating unique filenames
import shutil # for file copy
import string
import sys
import urllib.request # for making HTTP requests to download attachment URLs

def main(args):
	option_print_eta_progress = False
	if args.verbose is not None and args.verbose >= 1:
		option_print_eta_progress = True

	num_total_entries = 0
	num_total_bytes = 0
	datetime_start = datetime.now()
	if option_print_eta_progress:
		# Count through the file
		with open(args.incsv.name, mode='r', newline='') as fin:
			csvreader = csv.DictReader(fin, strict=True)
			for row in csvreader:
				num_total_entries += 1
				num_total_bytes += int(row["bytes_size"])

	with open(args.incsv.name, mode='r', newline='') as fin:
		entry_count = 0
		byte_count = 0
		csvreader = csv.DictReader(fin, strict=True)
		for row in csvreader:
			msgid = row["msgid"]
			attid = row["attachment_id"]
			destination_path = get_unique_path(args.outdir, row["filename"])

			print("msgid ", msgid, ": attid ", attid, " -> ",
				sep='', end='', flush=True)
			# Download attachment
			try:
				download_url(row["attachment_url"], destination_path, args.user_agent)
			except urllib.error.HTTPError as err:
				if err.code == 403: #Forbidden
					if args.user_agent is None:
						print("\n\t" + str(err))
						print("Please consider using the --user-agent option.")
						return

					# Retry via attachment_proxy_url
					try:
						download_url(row["attachment_proxy_url"], destination_path, args.user_agent)
					except urllib.error.HTTPError as err2:
						if err2.code == 502: #Bad Gateway
							print("DELETED ("+str(err2)+")")
							# File's gone. Can't do anything about it.
						else:
							raise
					else:
						print("(by proxy)", destination_path.name)
				else:
					raise
			else:
				print(destination_path.name)

			if option_print_eta_progress:
				entry_count += 1
				byte_count += int(row["bytes_size"])

				current_run_time = datetime.now() - datetime_start
				estimated_run_time_bybyte = current_run_time * num_total_bytes / byte_count
				approx_eta_bybyte = estimated_run_time_bybyte - current_run_time
				estimated_run_time_byentry = current_run_time * num_total_entries / entry_count
				approx_eta_byentry = estimated_run_time_byentry - current_run_time
				approx_eta_str = ""
				if approx_eta_bybyte < approx_eta_byentry:
					approx_eta_str = str(approx_eta_bybyte)+'~'+str(approx_eta_byentry)
				else:
					approx_eta_str = str(approx_eta_byentry)+'~'+str(approx_eta_bybyte)
				# hey it's about as good as a Windows file operation eta but it's better than nothing

				print('\t', entry_count, '/', num_total_entries, " eta~", approx_eta_str,
					sep='')
			#end-if: option_print_eta_progress

			print(end='', flush=True)
		#end-for: row .csv processing loop

def get_unique_path(directory, filename):
	"""Return a unique file path by appending random alphanumeric characters to the given filename, as necesary."""
	CHARPOOL = string.ascii_letters + string.digits # Random chars to create suffix from

	directory_path = Path(directory)
	filename_path = Path(filename)
	basename = filename_path.stem + '_'
	extension = filename_path.suffix

	# '/' operator joins path-like objects.
	path_candidate = directory_path / (basename + extension)
	# Append random characters until a non-existent path is found.
	while path_candidate.exists():
		basename += random.choice(CHARPOOL)
		path_candidate = directory_path / (basename + extension)

	return path_candidate

def download_url(url, destination_path, useragent=None):
	request = urllib.request.Request(url,
		headers={"User-Agent": useragent} if useragent is not None else {})
	with urllib.request.urlopen(request) as response, open(destination_path, mode="xb") as fout:
		shutil.copyfileobj(response, fout)

#src: https://stackoverflow.com/q/11415570 ("directory path types with argparse")
def writable_dir(prospective_dir):
	if not os.path.isdir(prospective_dir):
		raise argparse.ArgumentTypeError("Not a directory: "+prospective_dir)
	if not os.access(prospective_dir, os.W_OK):
		raise argparse.ArgumentTypeError("Write access denied: "+prospective_dir)

	return prospective_dir

if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--user-agent", "-ua",
		help="user agent to use when downloading from Discord. "
			+"Discord will issue an HTTP 403 error to requests from the default user agent 'Python-urllib/x.y'.")
	parser.add_argument("--verbose", "-v",
		action="count",
		help="verbosity level, increase by specifying multiple times: "
			+"0-display current processed msgid, "
			+"1-display overall progress/eta, "
			+"2-debug info")
	parser.add_argument("incsv",
		type=argparse.FileType('r'),
		help=".csv file generated by extract_log_attachments_urls_to_csv.py")
	parser.add_argument("outdir",
		type=writable_dir,
		help="attachment output directory")
	parsed_args = parser.parse_args()

	if parsed_args.incsv.name == "<stdin>":
		sys.exit("Error: stdin not supported")

	main(parsed_args)
